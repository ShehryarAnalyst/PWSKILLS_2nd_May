{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6345354e",
   "metadata": {},
   "source": [
    "## Assignment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e27ec1",
   "metadata": {},
   "source": [
    "__Q1. What is anomaly detection and what is its purpose?__\n",
    "\n",
    "A1. Anomaly detection refers to the process of identifying patterns or instances that deviate significantly from the norm or expected behavior within a dataset. Its purpose is to identify and flag unusual or suspicious data points that may indicate potential fraud, errors, or other exceptional events.\n",
    "\n",
    "__Q2. What are the key challenges in anomaly detection?__\n",
    "\n",
    "A2. Some key challenges in anomaly detection include dealing with imbalanced datasets where anomalies are rare, defining what constitutes normal behavior, adapting to evolving patterns of anomalies, handling high-dimensional data, and minimizing false positives and false negatives.\n",
    "\n",
    "__Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?__\n",
    "A3. In unsupervised anomaly detection, the algorithm learns the normal patterns from the data itself without any prior labeled information about anomalies. It aims to identify deviations from the learned normal behavior. On the other hand, supervised anomaly detection relies on labeled data that explicitly indicates normal and anomalous instances, enabling the algorithm to learn a classification model to distinguish between the two.\n",
    "\n",
    "__Q4. What are the main categories of anomaly detection algorithms?__\n",
    "\n",
    "A4. The main categories of anomaly detection algorithms include statistical methods (e.g., Gaussian distribution models), proximity-based methods (e.g., k-nearest neighbors), density-based methods (e.g., Local Outlier Factor), clustering-based methods (e.g., DBSCAN), and machine learning-based methods (e.g., Isolation Forest, one-class SVM).\n",
    "\n",
    "__Q5. What are the main assumptions made by distance-based anomaly detection methods?__\n",
    "\n",
    "A5. Distance-based anomaly detection methods assume that anomalies are located far away from the majority of the data points in the feature space. These methods typically use distance metrics, such as Euclidean distance, to measure the dissimilarity between data points and identify outliers based on their distance from the rest of the data.\n",
    "\n",
    "__Q6. How does the LOF algorithm compute anomaly scores?__\n",
    "\n",
    "A6. The LOF (Local Outlier Factor) algorithm computes anomaly scores based on the local density of data points. It compares the density of a data point to the densities of its neighboring points. If a point has a much lower density compared to its neighbors, it is likely to be an anomaly. The LOF score represents the degree of abnormality by comparing the local density of a point with that of its neighbors.\n",
    "\n",
    "__Q7. What are the key parameters of the Isolation Forest algorithm?__\n",
    "\n",
    "A7. The key parameters of the Isolation Forest algorithm are the number of trees (n_estimators) and the subsampling size (max_samples). The number of trees determines the number of isolation trees to be built, while the subsampling size controls the number of samples drawn from the data for each tree. These parameters affect the trade-off between anomaly detection accuracy and computational efficiency.\n",
    "\n",
    "__Q8. If a data point has only 2 neighbors of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?__\n",
    "\n",
    "A8. To compute the anomaly score using k-nearest neighbors (KNN) with K=10, we need to consider the number of neighbors within the specified radius. In this case, if the data point has only 2 neighbors of the same class within a radius of 0.5, its anomaly score would be relatively high. The exact scoring calculation may depend on the specific algorithm or scoring method being used.\n",
    "\n",
    "__Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?__\n",
    "\n",
    "A9. In the Isolation Forest algorithm, the anomaly score is computed based on the average path length of a data point in the isolation trees. The average path length represents the average number of edges traversed to isolate the data point. If a data point has an average path length of 5.0 compared to the average path length of the trees, it suggests that it is less likely to be an anomaly. However, the specific anomaly score calculation may depend on the algorithm's implementation and scoring methodology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
